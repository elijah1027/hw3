spark = SparkSession\
        .builder\
        .appName("PythonSort")\
        .getOrCreate()
#create or get a new spark session
lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])
#spark.read.text(sys.argv[1]) => read text file
#rdd.map(lambda r: r[0]) => this is a RDD object. apply lambda function to the lines in text file. Return list of tuple. 
Element i in this object is the i-th line in the text.
sortedCount = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (int(x), 1)).sortByKey()
#sortedCount = lines.flatMap(lambda x: x.split(' ')) => split lines into charaters(by space)
#.map(lambda x: (int(x), 1)) => convert elements into integer key and assign value 1 to each key
#.sortByKey() => for each key in the previous object, sort value in ascending order
output = sortedCount.collect()
#convert RDD to list and store into output
for (num, unitcount) in output:
        print(num)
#print the key